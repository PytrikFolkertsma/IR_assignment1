{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Unstructured and Structured Data: Assignment 1 ### \n",
    "\n",
    "The cell below contains the necessary code we were provided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 51.61398696899414 seconds.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import pyndri\n",
    "import os\n",
    "import re\n",
    "from subprocess import PIPE, run\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "token2id, id2token, _ = index.get_dictionary()\n",
    "\n",
    "def write_run(model_name, data, out_f, max_objects_per_query=sys.maxsize, skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.', subject_id)\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "\n",
    "def parse_topics(file_or_files, max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "    topics = collections.OrderedDict()\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).', topic_id, topics[topic_id], terms)\n",
    "            topics[topic_id] = terms\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "start_time = time.time()\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "total_terms = 0\n",
    "total_nr_docs = index.maximum_document() - index.document_base()\n",
    "ext_doc_ids = {}\n",
    "ext_to_int_doc_ids = {}\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    ext_doc_ids[int_doc_id] = ext_doc_id\n",
    "    ext_to_int_doc_ids[ext_doc_id] = int_doc_id\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [40 points] ### \n",
    "\n",
    "Specify the directory to trec eval in the first cell. The code in the second cell creates all necessary functions and the third cell contains the code for generating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec_eval_directory = '../trec_eval/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = 'results_task1/{}.run'.format(model_name)\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    # TODO: fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    for query_id in tokenized_queries:\n",
    "        query = tokenized_queries[query_id]\n",
    "        doc_ids = set([doc_id for term in query for doc_id in inverted_index[term].keys()])\n",
    "        data[query_id] = []\n",
    "        for doc_id in doc_ids:\n",
    "            score = 0\n",
    "            for term_id in query:\n",
    "                doc_term_freq = len(inverted_index.get(term_id, 0))\n",
    "                score += score_fn(doc_id, term_id, doc_term_freq)\n",
    "            data[query_id].append((score, ext_doc_ids[doc_id]))\n",
    "    \n",
    "    print('Retrieval took', time.time() - retrieval_start_time, 'seconds.')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "\n",
    "\n",
    "def get_tf(query_term_id, int_document_id=None):\n",
    "    \"\"\"\n",
    "    Returns term frequency for a document if a document_id is given,\n",
    "    or for the entire collection if no document_id is given.\n",
    "    \"\"\"\n",
    "    if int_document_id:\n",
    "        return float(inverted_index.get(query_term_id, 0).get(int_document_id, 0))\n",
    "    else:\n",
    "        return collection_frequencies.get(query_term_id, 0)\n",
    "\n",
    "def get_bg_prob(query_term_id, document_term_freq=None):\n",
    "    tf_col = get_tf(query_term_id)\n",
    "    bg_prob = tf_col/total_terms\n",
    "    return bg_prob\n",
    "    \n",
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "\n",
    "    # TODO implement the function\n",
    "    idf = np.log(total_nr_docs/document_term_freq)\n",
    "    tf = get_tf(query_term_id, int_document_id)\n",
    "    score = np.log(1 + tf) * idf\n",
    "    return score\n",
    "\n",
    "def bm25(int_document_id, query_term_id, document_term_freq, k1=1.2, b=0.75):\n",
    "    tf = get_tf(query_term_id, int_document_id)\n",
    "    doc_len = document_lengths[int_document_id]\n",
    "    term_score = ((k1+1) * tf)/(k1*((1-b)+b*(doc_len/avg_doc_length))+tf)\n",
    "    idf = np.log(total_nr_docs/document_term_freq)\n",
    "    score = term_score * idf\n",
    "    return score\n",
    "\n",
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq, l=0.1):\n",
    "    tf_doc = get_tf(query_term_id, int_document_id)\n",
    "    tf_col = get_tf(query_term_id)\n",
    "    doc_len = document_lengths[int_document_id]\n",
    "    probability = l*(tf_doc/doc_len)+(1-l)*(tf_col/total_terms)\n",
    "    return np.log(probability)\n",
    "\n",
    "def dirichlet_prior(int_document_id, query_term_id, document_term_freq, mu=1000):\n",
    "    tf = get_tf(query_term_id, int_document_id)\n",
    "    bg_prob = get_bg_prob(query_term_id, document_term_freq)\n",
    "    doc_len = document_lengths[int_document_id]\n",
    "    probability = (tf + mu*bg_prob)/(doc_len + mu)\n",
    "    return np.log(probability)\n",
    "\n",
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, delta=0.9):\n",
    "    bg_prob = get_bg_prob(query_term_id, document_term_freq)\n",
    "    tf = get_tf(query_term_id, int_document_id)\n",
    "    doc_len = document_lengths[int_document_id]\n",
    "    unique_terms = unique_terms_per_document[int_document_id]\n",
    "    probability = (max(tf-delta, 0)/doc_len)+(delta*unique_terms/doc_len)*bg_prob\n",
    "    return np.log(probability)\n",
    "\n",
    "def get_hyper_parameter_values():\n",
    "    filenames = ['jelinek_mercer', 'dirichlet_prior', 'absolute_discounting']\n",
    "    measure = 'ndcg_cut_10'\n",
    "    best_parameters = {}\n",
    "    data = []\n",
    "    names = []\n",
    "    print('\\nMeasure:', measure)\n",
    "    for lm in filenames:\n",
    "        files = [f for f in os.listdir('results_task1') if re.match(lm, f) and f != '.DS_Store']\n",
    "        for file in files:\n",
    "            result = run('./' + trec_eval_directory + 'trec_eval -m all_trec ap_88_89/qrel_validation results_task1/' + file + ' | grep -E \"^' + measure + '\\s\"', shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            score = result.stdout.split('\\t')[2].strip()\n",
    "            param = re.search(re.compile('\\d.\\d*'), file).group()\n",
    "            print(lm, param, score)\n",
    "            data.append(score)\n",
    "            names.append(lm + '_' + param)\n",
    "            if lm not in best_parameters:\n",
    "                best_parameters[lm] = {'best_param': param, 'score': score}\n",
    "                continue\n",
    "            if score > best_parameters[lm]['score']:\n",
    "                best_parameters[lm]['best_param'] = param\n",
    "                best_parameters[lm]['score'] = score\n",
    "\n",
    "    width = 1/1.5\n",
    "    plt.bar(range(len(data)), data, 1/1.5, align='edge', color='gray')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,0.5])\n",
    "    plt.xticks(range(len(data)), names, rotation=60)\n",
    "    plt.ylabel('NDCG@10')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/' + measure + '.png')\n",
    "    plt.title('Scores of ' + measure)\n",
    "    plt.close() \n",
    "\n",
    "    print(best_parameters)\n",
    "\n",
    "def get_trec_eval_results(measures, output_dir, directory=None):\n",
    "    \"\"\"\n",
    "    Prints TREC Eval results and writes results to file.\n",
    "    \"\"\"\n",
    "    if directory:\n",
    "        files = sorted([f for f in os.listdir(directory) if f != '.DS_Store'])\n",
    "    else:\n",
    "        directory = 'results_task1/'\n",
    "        files = ['tfidf.run', 'bm25.run', 'jelinek_mercer0.1.run', 'dirichlet_prior1000.run', 'absolute_discounting0.9.run']\n",
    "    \n",
    "    for measure in measures:\n",
    "        results = []\n",
    "        firstline = True\n",
    "        output = open(output_dir + measure + '.txt', 'w')\n",
    "        print('\\nMeasure:', measure)\n",
    "        for file in files:\n",
    "            result = run('./' + trec_eval_directory + 'trec_eval -m all_trec ap_88_89/qrel_test ' + directory + file + ' -q | grep -E \"^' + measure + '\\s\"', shell=True, stdout=PIPE, stderr=PIPE, universal_newlines=True)\n",
    "            scores = [float(line.split('\\t')[2]) for line in result.stdout.split('\\n')[:-2]] #[:-2] because last element is empty and second last element is score for all\n",
    "            score = round(sum(scores)/len(scores),3)\n",
    "            if firstline:\n",
    "                queries = [line.split('\\t')[1] for line in result.stdout.split('\\n')[:-2]]\n",
    "                output.write(measure + '\\t' + '\\t'.join(queries) + '\\n')\n",
    "                firstline = False\n",
    "            print(file, score)\n",
    "            output.write(file + '\\t' + '\\t'.join(str(x) for x in scores) + '\\n')\n",
    "        output.close() \n",
    "\n",
    "def test_methods(file):\n",
    "    \"\"\"\n",
    "    Performs two-tailed Student t-test between MAP results.\n",
    "    \"\"\"\n",
    "    lines = [[line.split('\\t')[0],[float(score) for score in line.split('\\t')[1:]]] for line in open(file).readlines()]\n",
    "    count = 0\n",
    "    results = []\n",
    "    for i in range(1, len(lines)):\n",
    "        for e in range(i+1, len(lines)):\n",
    "            count += 1\n",
    "            t = stats.ttest_rel(lines[i][1], lines[e][1])\n",
    "            print(lines[i][0], lines[e][0], t.pvalue)\n",
    "    print('Number of comparisons:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper parameter values\n",
      "\n",
      "Measure: ndcg_cut_10\n",
      "jelinek_mercer 0.9 0.3676\n",
      "jelinek_mercer 0.5 0.3823\n",
      "jelinek_mercer 0.1 0.3991\n",
      "dirichlet_prior 1000 0.4002\n",
      "dirichlet_prior 500 0.4055\n",
      "dirichlet_prior 1500 0.4026\n",
      "absolute_discounting 0.1 0.3614\n",
      "absolute_discounting 0.5 0.3768\n",
      "absolute_discounting 0.9 0.3950\n",
      "{'jelinek_mercer': {'best_param': '0.1', 'score': '0.3991'}, 'dirichlet_prior': {'best_param': '500', 'score': '0.4055'}, 'absolute_discounting': {'best_param': '0.9', 'score': '0.3950'}}\n",
      "\n",
      "Getting TREC Eval results on test set\n",
      "\n",
      "Measure: ndcg_cut_10\n",
      "tfidf.run 0.417\n",
      "bm25.run 0.409\n",
      "jelinek_mercer0.1.run 0.349\n",
      "dirichlet_prior1000.run 0.414\n",
      "absolute_discounting0.9.run 0.386\n",
      "\n",
      "Measure: map_cut_1000\n",
      "tfidf.run 0.216\n",
      "bm25.run 0.217\n",
      "jelinek_mercer0.1.run 0.189\n",
      "dirichlet_prior1000.run 0.213\n",
      "absolute_discounting0.9.run 0.203\n",
      "\n",
      "Measure: P_5\n",
      "tfidf.run 0.432\n",
      "bm25.run 0.413\n",
      "jelinek_mercer0.1.run 0.345\n",
      "dirichlet_prior1000.run 0.413\n",
      "absolute_discounting0.9.run 0.398\n",
      "\n",
      "Measure: recall_1000\n",
      "tfidf.run 0.651\n",
      "bm25.run 0.652\n",
      "jelinek_mercer0.1.run 0.62\n",
      "dirichlet_prior1000.run 0.635\n",
      "absolute_discounting0.9.run 0.626\n",
      "\n",
      "Significance testing\n",
      "tfidf.run bm25.run 0.739269264159\n",
      "tfidf.run jelinek_mercer0.1.run 0.000116294645219\n",
      "tfidf.run dirichlet_prior1000.run 0.708264912572\n",
      "tfidf.run absolute_discounting0.9.run 0.00812534944018\n",
      "bm25.run jelinek_mercer0.1.run 3.00440427347e-12\n",
      "bm25.run dirichlet_prior1000.run 0.279238823177\n",
      "bm25.run absolute_discounting0.9.run 3.59259521498e-07\n",
      "jelinek_mercer0.1.run dirichlet_prior1000.run 2.57792196534e-08\n",
      "jelinek_mercer0.1.run absolute_discounting0.9.run 0.000421706512776\n",
      "dirichlet_prior1000.run absolute_discounting0.9.run 0.00637682466709\n",
      "Number of comparisons: 10\n"
     ]
    }
   ],
   "source": [
    "# combining the two functions above: \n",
    "# run_retrieval('tfidf', tfidf)\n",
    "# run_retrieval('bm25', bm25)\n",
    "# run_retrieval('jelinek_mercer0.1', jelinek_mercer)\n",
    "# run_retrieval('dirichlet_prior1000', dirichlet_prior)\n",
    "# run_retrieval('absolute_discounting0.9', absolute_discounting)\n",
    "\n",
    "# TODO implement tools to help you with the analysis of the results.\n",
    "print('Best hyper parameter values')\n",
    "get_hyper_parameter_values()\n",
    "print('\\nGetting TREC Eval results on test set')\n",
    "get_trec_eval_results([\"ndcg_cut_10\", \"map_cut_1000\", \"P_5\", \"recall_1000\"], 'trec_eval_results_task1/')\n",
    "print('\\nSignificance testing')\n",
    "test_methods('trec_eval_results_task1/map_cut_1000.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [20 points] ###\n",
    "\n",
    "The code below generates our results for the second task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Measure: ndcg_cut_10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Measure: map_cut_1000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "from gensim import corpora, models, similarities\n",
    "import collections\n",
    "from scipy import stats\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "corpus = IndriCorpus(index, dictionary)\n",
    "\n",
    "document_candidates = {}\n",
    "\n",
    "for line in open('results_task1/tfidf.run'):\n",
    "    line = line.split(' ')\n",
    "    query = int(line[0])\n",
    "    if query not in document_candidates:\n",
    "        document_candidates[query] = []\n",
    "    document_candidates[query].append(ext_to_int_doc_ids[line[2]])\n",
    "\n",
    "class IndriCorpus(gensim.interfaces.CorpusABC):\n",
    "\n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        assert isinstance(index, pyndri.Index)\n",
    "\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "        self.max_documents = max_documents\n",
    "\n",
    "    def _maximum_document(self):\n",
    "        if self.max_documents is None:\n",
    "            return self.index.maximum_document()\n",
    "        else:\n",
    "            return min(\n",
    "                self.max_documents + self.index.document_base(),\n",
    "                self.index.maximum_document())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, tokens = self.index.document(int_doc_id)\n",
    "\n",
    "            # Compared to IndriSentences, the only difference is the\n",
    "            # switching of tuple(self.dictionary[token_id] ...) by\n",
    "            # sorted(collections.Counter(token_id ...).items()).\n",
    "            yield sorted(collections.Counter(\n",
    "                token_id\n",
    "                for token_id in tokens\n",
    "                if token_id > 0 and token_id in self.dictionary).items())\n",
    "\n",
    "def generate_lsm_results(run_out_path, model_name, k):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('creating', model_name, 'model')\n",
    "\n",
    "    if model_name == 'lda':\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word=dictionary, minimum_probability=0.0)\n",
    "    else:\n",
    "        model = gensim.models.lsimodel.LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
    "    \n",
    "    print('calculating query document similarity')\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for query_id in queries:\n",
    "        data[query_id] = []\n",
    "        query_bow = dictionary.doc2bow(queries[query_id].lower().split())\n",
    "        query_vec = model[query_bow]\n",
    "        query = tokenized_queries[query_id]\n",
    "        docs = document_candidates[int(query_id)]\n",
    "        for doc_id in docs:\n",
    "            doc_token_ids = index.document(doc_id)[1]\n",
    "            doc_bow = dictionary.doc2bow(doc_token_ids)\n",
    "            doc_vec = model[doc_bow]\n",
    "            if model_name == 'lda':\n",
    "                score = 0 - stats.entropy(query_vec, doc_vec)[1]\n",
    "            else:\n",
    "                score = gensim.matutils.cossim(query_vec, doc_vec)\n",
    "            data[query_id].append((score, ext_doc_ids[doc_id]))\n",
    "            \n",
    "    print('LSM took', time.time() - start_time, 'seconds')\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "        model_name='LDA'+str(k),\n",
    "        data=data,\n",
    "        out_f=f_out,\n",
    "        max_objects_per_query=1000)\n",
    "\n",
    "#generate lsi and lda models with 2 topics\n",
    "# generate_lsm_results('results_task2/LSI_2.run', 'lsi', 2)\n",
    "# generate_lsm_results('results_task2/LDA_2.run', 'lda', 2)\n",
    "\n",
    "get_trec_eval_results([\"ndcg_cut_10\", \"map_cut_1000\"], 'trec_eval_results_task2/', directory='results_task2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [10 points] ###\n",
    "\n",
    "The code below generates our results for the third task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [10 points] ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
